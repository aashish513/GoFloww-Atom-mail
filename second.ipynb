{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1540ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from typing import List, Optional\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Define structure of a single message\n",
    "class Message(BaseModel):\n",
    "    sender_name: str\n",
    "    sender_email: str\n",
    "    message: str\n",
    "    timestamp: Optional[str] = None  # ISO string, optional\n",
    "\n",
    "# Main request body\n",
    "class EmailRequest(BaseModel):\n",
    "    conversation_thread: List[Message]  # Full conversation thread\n",
    "    reply_style: str  # e.g., \"formal\", \"friendly\", etc.\n",
    "    prompt: Optional[str] = None       # Optional custom prompt to override\n",
    "    new_message: Optional[str] = None  # Optional new message at the end\n",
    "\n",
    "@app.post(\"/generate-email\")\n",
    "def generate_email(req: EmailRequest):\n",
    "    # Build the conversation thread text\n",
    "    thread_text = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e85441df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation_thread': [{'sender_name': 'Alice',\n",
       "   'sender_email': 'alice@example.com',\n",
       "   'message': 'Hey Bob, can you share the report by Monday?'},\n",
       "  {'sender_name': 'Bob',\n",
       "   'sender_email': 'bob@example.com',\n",
       "   'message': \"Sure, I'll send it by end of day.\"},\n",
       "  {'sender_name': 'Alice',\n",
       "   'sender_email': 'alice@example.com',\n",
       "   'message': 'Thanks! Please include the latest sales figures too.'}],\n",
       " 'reply_style': 'formal',\n",
       " 'new_message': 'Here’s the final version of the report. Let me know if anything else is needed.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"conversation_thread\": [\n",
    "    {\n",
    "      \"sender_name\": \"Alice\",\n",
    "      \"sender_email\": \"alice@example.com\",\n",
    "      \"message\": \"Hey Bob, can you share the report by Monday?\"\n",
    "    },\n",
    "    {\n",
    "      \"sender_name\": \"Bob\",\n",
    "      \"sender_email\": \"bob@example.com\",\n",
    "      \"message\": \"Sure, I'll send it by end of day.\"\n",
    "    },\n",
    "    {\n",
    "      \"sender_name\": \"Alice\",\n",
    "      \"sender_email\": \"alice@example.com\",\n",
    "      \"message\": \"Thanks! Please include the latest sales figures too.\"\n",
    "    }\n",
    "  ],\n",
    "  \"reply_style\": \"formal\",\n",
    "  \"new_message\": \"Here’s the final version of the report. Let me know if anything else is needed.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f709d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Message structure\n",
    "class Message(BaseModel):\n",
    "    sender_name: str\n",
    "    sender_email: str\n",
    "    message: str\n",
    "    timestamp: Optional[str] = None\n",
    "\n",
    "# Main request model\n",
    "class EmailRequest(BaseModel):\n",
    "    conversation_thread: List[Message]\n",
    "    reply_style: str  # e.g., \"formal\", \"friendly\", etc.\n",
    "\n",
    "@app.post(\"/generate-email\")\n",
    "def generate_email(req: EmailRequest):\n",
    "    # Separate latest message from previous history\n",
    "    *history_messages, latest_message = req.conversation_thread\n",
    "\n",
    "    # Build history text\n",
    "    history_text = \"\"\n",
    "    for msg in history_messages:\n",
    "        history_text += f\"{msg.sender_name} ({msg.sender_email}): {msg.message}\\n\"\n",
    "\n",
    "    # Add the message that needs to be replied to\n",
    "    last_sender = latest_message.sender_name\n",
    "    last_email = latest_message.sender_email\n",
    "    last_msg = latest_message.message\n",
    "\n",
    "    # Construct prompt\n",
    "    prompt = (\n",
    "        f\"You are an AI email assistant.\\n\"\n",
    "        f\"Reply style: {req.reply_style}\\n\\n\"\n",
    "        f\"Conversation history:\\n{history_text.strip()}\\n\\n\"\n",
    "        f\"The latest message from {last_sender} ({last_email}) is:\\n\"\n",
    "        f\"\\\"{last_msg}\\\"\\n\\n\"\n",
    "        f\"Generate a clear, well-structured, and {req.reply_style.lower()} reply based on the entire conversation.\"\n",
    "    )\n",
    "\n",
    "    # Generate response\n",
    "    response = ollama.chat(\n",
    "        model=\"gemma:1b-instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return {\"response\": response[\"message\"][\"content\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "679f3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_auto_reply(conversation_thread, reply_style=\"formal\", model=\"gemma:2b\"):\n",
    "    \"\"\"\n",
    "    Generates an AI email reply based on a conversation thread.\n",
    "\n",
    "    Parameters:\n",
    "        conversation_thread (list of dict): List of messages with keys: sender_name, sender_email, message.\n",
    "        reply_style (str): Style of the reply e.g., \"formal\", \"friendly\", etc.\n",
    "        model (str): Ollama model to use.\n",
    "\n",
    "    Returns:\n",
    "        str: AI-generated reply message.\n",
    "    \"\"\"\n",
    "    if len(conversation_thread) < 1:\n",
    "        return \"Conversation thread is empty.\"\n",
    "\n",
    "    *history_messages, latest_message = conversation_thread\n",
    "\n",
    "    # Build conversation history\n",
    "    history_text = \"\"\n",
    "    for msg in history_messages:\n",
    "        history_text += f\"{msg['sender_name']} ({msg['sender_email']}): {msg['message']}\\n\"\n",
    "\n",
    "    # Last message to reply to\n",
    "    last_sender = latest_message[\"sender_name\"]\n",
    "    last_email = latest_message[\"sender_email\"]\n",
    "    last_msg = latest_message[\"message\"]\n",
    "\n",
    "    # Construct prompt\n",
    "    prompt = (\n",
    "        f\"You are an AI email assistant.\\n\"\n",
    "        f\"Reply style: {reply_style}\\n\\n\"\n",
    "        f\"Conversation history:\\n{history_text.strip()}\\n\\n\"\n",
    "        f\"The latest message from {last_sender} ({last_email}) is:\\n\"\n",
    "        f\"\\\"{last_msg}\\\"\\n\\n\"\n",
    "        f\"Generate a clear, well-structured, and {reply_style.lower()} reply based on the entire conversation.\"\n",
    "    )\n",
    "\n",
    "    # Call Ollama\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response[\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396fece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-generated reply:\n",
      "\n",
      "Dear Team Lead,\n",
      "\n",
      "I understand that the design document is a priority, and I will work diligently to complete it by Thursday. I have already started working on the initial draft and will prioritize the inclusion of the flow diagrams in the next iteration.\n",
      "\n",
      "Please let me know if you have any further specific instructions or if you would like to review the draft document at any stage.\n",
      "\n",
      "Thank you for your understanding and support.\n",
      "\n",
      "Sincerely,\n",
      "John\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sample_conversation = [\n",
    "        {\n",
    "            \"sender_name\": \"Team Lead\",\n",
    "            \"sender_email\": \"lead@example.com\",\n",
    "            \"message\": \"Hi John, please make sure the design document is ready by Thursday.\"\n",
    "        },\n",
    "        {\n",
    "            \"sender_name\": \"John\",\n",
    "            \"sender_email\": \"john@example.com\",\n",
    "            \"message\": \"Sure, I’ll finish the initial draft by tomorrow.\"\n",
    "        },\n",
    "        {\n",
    "            \"sender_name\": \"Team Lead\",\n",
    "            \"sender_email\": \"lead@example.com\",\n",
    "            \"message\": \"Great. Also, include the flow diagrams.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    reply = generate_auto_reply(sample_conversation, reply_style=\"formal\")\n",
    "    print(\"AI-generated reply:\\n\")\n",
    "    print(reply)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
